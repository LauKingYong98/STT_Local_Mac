{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e9baf2-7467-4196-bfba-2f245022b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "MPS available: True\n",
      "MPS backend exists: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "# Print PyTorch device information to verify MPS availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS backend exists: {torch.backends.mps.is_built()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8eaf01-11e6-4056-9e54-db7e06fe9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    def __init__(self):\n",
    "        # Initialize device - use MPS if available\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load model and processor\n",
    "        model_id = \"distil-whisper/distil-large-v2\"\n",
    "        \n",
    "        print(\"Loading model...\")\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(\"Loading processor...\")\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "        \n",
    "        print(\"Creating pipeline...\")\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.processor.tokenizer,\n",
    "            feature_extractor=self.processor.feature_extractor,\n",
    "            max_new_tokens=128,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=16,\n",
    "            return_timestamps=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device=self.device,\n",
    "        )\n",
    "        print(\"Setup complete!\")\n",
    "\n",
    "    def download_audio(self, youtube_url, output_path=\"audio\"):\n",
    "        \"\"\"Download audio from YouTube URL\"\"\"\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "            \n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'outtmpl': f'{output_path}/%(title)s.%(ext)s',\n",
    "        }\n",
    "        \n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(youtube_url, download=True)\n",
    "            audio_path = f\"{output_path}/{info['title']}.mp3\"\n",
    "            return audio_path\n",
    "\n",
    "    def transcribe(self, youtube_url):\n",
    "        \"\"\"Transcribe audio from YouTube URL\"\"\"\n",
    "        try:\n",
    "            # Get notebook's directory\n",
    "            notebook_dir = os.getcwd()\n",
    "            print(f\"Working directory: {notebook_dir}\")\n",
    "            \n",
    "            # Download audio\n",
    "            print(\"Downloading audio...\")\n",
    "            audio_path = self.download_audio(youtube_url)\n",
    "            \n",
    "            # Transcribe\n",
    "            print(\"Transcribing...\")\n",
    "            result = self.pipe(\n",
    "                audio_path,\n",
    "                generate_kwargs={\"language\": \"en\", \"task\": \"transcribe\"}\n",
    "            )\n",
    "            \n",
    "            # Save transcription\n",
    "            output_file = os.path.join(notebook_dir, \"transcription.txt\")\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(result[\"text\"])\n",
    "            print(f\"\\nTranscription saved to: {output_file}\")\n",
    "            \n",
    "            # Clean up audio file\n",
    "            os.remove(audio_path)\n",
    "            \n",
    "            return result[\"text\"]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8902d00-87e8-46f8-951f-592d19c52404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading model...\n",
      "Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipeline...\n",
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/whisper-env/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create transcriber instance\n",
    "transcriber = AudioTranscriber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fdf7da7-3636-4c49-a065-d60d127e0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/jonathanlau/Downloads/repos/STT_Local_Mac\n",
      "Downloading audio...\n",
      "[youtube] Extracting URL: https://youtu.be/Tn-XvYG9x7w?si=orngdjVQsMOAjJn_\n",
      "[youtube] Tn-XvYG9x7w: Downloading webpage\n",
      "[youtube] Tn-XvYG9x7w: Downloading ios player API JSON\n",
      "[youtube] Tn-XvYG9x7w: Downloading mweb player API JSON\n",
      "[youtube] Tn-XvYG9x7w: Downloading m3u8 information\n",
      "[info] Tn-XvYG9x7w: Downloading 1 format(s): 251\n",
      "[download] audio/Get Abs In 60 Days (Using Science).webm has already been downloaded\n",
      "[download] 100% of    9.88MiB\n",
      "[ExtractAudio] Destination: audio/Get Abs In 60 Days (Using Science).mp3\n",
      "Deleting original file audio/Get Abs In 60 Days (Using Science).webm (pass -k to keep)\n",
      "Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/whisper-env/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription saved to: /Users/jonathanlau/Downloads/repos/STT_Local_Mac/transcription.txt\n",
      "\n",
      "Transcription Preview (first 500 characters):\n",
      " What does your midsection look like right now? For reference, this is what it would look like at 50% body fat. At 40%, your waistline is smaller, but your abs are still buried. At 30%, your stomach is much flatter, but you still don't have visible. Once you get to 20%, that's where your abs become visible. And at 10% body fat, you'll have a well-defined, assuming you've developed your abs through proper training, which we'll get to. At 6% body fat, you'd be truly shredded, lean enough for a pro...\n"
     ]
    }
   ],
   "source": [
    "# Test with a short YouTube video (replace with your URL)\n",
    "youtube_url = \"https://youtu.be/Tn-XvYG9x7w?si=orngdjVQsMOAjJn_\"\n",
    "transcription = transcriber.transcribe(youtube_url)\n",
    "\n",
    "if transcription:\n",
    "    print(\"\\nTranscription Preview (first 500 characters):\")\n",
    "    print(transcription[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7afe98ba-bdba-4fe3-815e-2283160ae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing summarization model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a4176ae0f943f1a95d472a800e8f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e44232f74c4fba815878e9e3b5d0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d55fdf28c6427a949349d6bd934d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdda88dc3fd545bebf29be3eea7c3718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b787c8e77ed54533a9fad6e80f796148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a507cd789eb640eabc3953334c5db0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization model ready!\n",
      "Processing text of length: 9304 characters\n",
      "Summarizing chunk 1/10...\n",
      "Summarizing chunk 2/10...\n",
      "Summarizing chunk 3/10...\n",
      "Summarizing chunk 4/10...\n",
      "Summarizing chunk 5/10...\n",
      "Summarizing chunk 6/10...\n",
      "Summarizing chunk 7/10...\n",
      "Summarizing chunk 8/10...\n",
      "Summarizing chunk 9/10...\n",
      "Summarizing chunk 10/10...\n",
      "\n",
      "Summary saved to: /Users/jonathanlau/Downloads/repos/STT_Local_Mac/transcription_summary.txt\n",
      "\n",
      "Summary Preview:\n",
      "At 6% body fat, you'd be truly shredded, lean enough for a pro-bodybuilding competition. Once you get to 20%, that's where your abs become visible. Most men want to be between 10 and 20% body tofact. It's popular these days to say that ab training is a waste of time. If you're not lean enough, you won't see the definition, anyway. I'm going to explain how to get your abs to pop regardless of the genetic hand you were dealt by using three science-based tools. If you build up your abdominal muscles through proper hypertrophy training, they will pop more just like any other muscle. To build your best six-pack, you really only need two exercises. One weight-loaded crunch, and one leg raise. If you don't have a cable, you can do plate-weighted crunches by holding a plate against your chest and crunching down hard on your six-pack. Do these for three sets of 10 to 12 reps twice per week. Anti-rotation movements can be great for targeting the transverse abdominis and obelics, but they won't hit the six-pack muscles whatever fits your schedule best. Research shows that combining weight training and cardio leads to smaller wastes than just weight training. Keep in mind that these calories and macros won't work perfectly for everyone. Try to prioritize minimally processed nutritious whole foods over highly processed, less nutritious junk food most of the time. For a sustainable fat loss, you want to lose about 0.5 to 1% of your body weight per week. While the scale can be a useful tool, it can also be misleading, especially if you're building muscle. It's common for people to have too lean of an end goal. Once you dip below the 8 to 10% zone for men, you'll start experiencing low energy, extreme hunger and reduced libido. Taking monthly waste measurements can help show that you are in fact losing fat. It's one of the only legal supplements that actually offers a meaningful boost in both strength and muscle mass. Decades of research have shown no negative side effects. It doesn't seem to matter when you take it and you don't need to cycle on and off. This is my own personal transformation where I lost 30 pounds by using the app's algorithms exclusively. I didn't manually adjust a single thing. I just followed whatever the app told me. That was also extremely fast to learn and easy to use. And you can cancel any the app.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "class TranscriptionSummarizer:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing summarization model...\")\n",
    "        # We'll use facebook/bart-large-cnn as it's optimized for summarization\n",
    "        self.summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"facebook/bart-large-cnn\",\n",
    "            device=0 if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        )\n",
    "        print(\"Summarization model ready!\")\n",
    "    \n",
    "    def summarize_file(self, file_path, max_length=130, min_length=30):\n",
    "        \"\"\"\n",
    "        Reads a text file and generates a summary.\n",
    "        \n",
    "        Parameters:\n",
    "            file_path (str): Path to the text file\n",
    "            max_length (int): Maximum length of the summary in tokens\n",
    "            min_length (int): Minimum length of the summary in tokens\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated summary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the transcription file\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            print(f\"Processing text of length: {len(text)} characters\")\n",
    "            \n",
    "            # For longer texts, we need to chunk them as BART has a token limit\n",
    "            # Most models can handle around 1024 tokens at once\n",
    "            chunks = self._chunk_text(text)\n",
    "            \n",
    "            # Generate summary for each chunk and combine\n",
    "            summaries = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "                summary = self.summarizer(\n",
    "                    chunk,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    do_sample=False\n",
    "                )[0]['summary_text']\n",
    "                summaries.append(summary)\n",
    "            \n",
    "            # Combine summaries\n",
    "            final_summary = \" \".join(summaries)\n",
    "            \n",
    "            # Save the summary\n",
    "            summary_path = file_path.replace('.txt', '_summary.txt')\n",
    "            with open(summary_path, 'w') as f:\n",
    "                f.write(final_summary)\n",
    "            \n",
    "            print(f\"\\nSummary saved to: {summary_path}\")\n",
    "            return final_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during summarization: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _chunk_text(self, text, max_chunk_size=1000):\n",
    "        \"\"\"\n",
    "        Splits text into smaller chunks while trying to maintain sentence integrity.\n",
    "        This helps handle longer texts that exceed model's token limit.\n",
    "        \"\"\"\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_size = len(sentence)\n",
    "            if current_size + sentence_size > max_chunk_size and current_chunk:\n",
    "                chunks.append('. '.join(current_chunk) + '.')\n",
    "                current_chunk = [sentence]\n",
    "                current_size = sentence_size\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_size\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "# Let's use our summarizer\n",
    "summarizer = TranscriptionSummarizer()\n",
    "\n",
    "# Path to your transcription file (adjust if needed)\n",
    "transcription_path = os.path.join(os.getcwd(), \"transcription.txt\")\n",
    "\n",
    "# Generate and display summary\n",
    "summary = summarizer.summarize_file(transcription_path)\n",
    "if summary:\n",
    "    print(\"\\nSummary Preview:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc8390-ddf4-4457-972c-e6fdbdbe9714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
